\documentclass{article}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{subfigure}

% page settings
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1}



%配置区
\newcommand{\courseName}{Pattern Recognition}
\newcommand{\homeworkTitle}{第六次作业}
\newcommand{\studentName}{牛李金梁}%姓名
\newcommand{\studentId}{201928014629008}%学号


\newcommand{\question}[1]{\section*{Question #1}}
\renewcommand{\part}[1]{\subsection*{(#1)}}


% Header 页眉
\pagestyle{fancy}
\lhead{\studentName}
\rhead{\courseName:\homeworkTitle}
\cfoot{\thepage}

\title{
    \vspace{2in}
    %\textmd{\textbf{\courseName}:\homeworkTitle}\\
    \textmd{\textbf{\courseName}}\\
    \textmd{\homeworkTitle}\\ %cy new add
    \vspace{0.1in}
    \large{\studentName}\\
    \large{\studentId}\\
    \vspace{3in}
}

\begin{document}


\maketitle
\date{}
\pagebreak

\section*{第一部分：问题、 计算与证明}
\section*{1 Adaboost算法的设计思想}
寻找弱分类器要比寻找精确的分类规则要简单得多。
Adaboost通过改变训练样本的权重反复学习得到一系列弱分类器，
并将这些弱分类器进行组合来作为一个强分类器。

在反复学习中，提高那些前一轮弱分类器分错的样本的权重，降低已被正确分类的样本的权
重，则错分的样本将在下一轮弱分类器中得到更多关注。

弱分类器的组合则采用弱分类器加权表决的方法，加大分类错误率较
小的弱分类器的权重，使其在表决中起更大的作用。

\section*{2 模型选择的基本原则}
对于在训练集上表现同样良好的2个分类器，更倾向于简单的那个。
\begin{itemize}
	\item \pmb{没有免费的午餐定理}
	对寻找代价函数极值的算法，在平均到所有可能的代价函数上时其表现恰好相同。
	不存在一个与具体应用无关的、普遍适用的最优分类器。
	算法必须要引入一些与问题领域有关的假设。
	要想在某些指标上得到性能的提高，必须在另一些指标上付出相应的代价。
	\item \pmb{丑小鸭定理} 不存在分类的客观标准,一切分类的标准都是主观的。
	不存在与问题无关的最优的特征/属性集合。
	不存在与问题无关的模式之间的相似性度量 。
	\item \pmb{Occam剃刀原理}如无必要，勿增实体。
	如果对训练数据分类的效果相同，简单的分类器往往优于复杂的分类器。
	相比复杂的假设，我们更倾向于选择简单的、参数少的假设。
	\item \pmb{最小描述长度原理}我们必须使模型的算法复杂度以及与该模型相适应
	的训练数据的描述长度之和最小。即应该选择尽可能简单的分类器或模型。
\end{itemize}

\section*{3 分类器集成的基本方法}
常用技术手段有处理训练数据、特征、类别标号，改进学习方法。
分类器集成算法可以按照基本分类器类型是否相同分为异态集成和同态集成。
典型的异态集成为层叠泛化，把前一层的输出作为这一层的输入。
集成算法也可以按照训练数据处理方式分为Bagging,Random subspace, Boosting/adaboost, 随机森林。
\begin{itemize}
	\item \pmb{Bagging} 训练一组基分类器，每个基分类器通过一个boostrap训练样本集来训练，获得基本分类
	器之后，bagging通过投票进行统计，被投票最多的类则确定为预测类。
	\item \pmb{Random Subspace} 对每一个分类器，选择部分子特征来构建
	一个训练集合，同时学习一个分类器。对于新样本，通过多数投票法来预测其类别。
	\item \pmb{Adaboost} 从弱学习算法出发，反复学习，得到一系列弱分类器；然后组合这些弱分类器，构成一
	个强分类器。每一轮中提高被前一轮弱分类器分错的样本的权重，降低已经被正确分类的样本的权重，
	错分的样本将在下一轮弱分类器中得到更多的关注。
\end{itemize}

\section*{4 Hard-Margin SVM的优化目标}
线性SVM判别函数形式为：
\begin{align*}
	f\left( {\pmb{x},\pmb{\omega},b} \right) = 
	sign\left( {{\pmb{\omega} ^T}\pmb{x} + b} \right)
\end{align*}
任一数据点到该判别面的距离为：
\begin{align*}
	d\left( \pmb{x} \right) 
	= \frac{{\left| {{\pmb{\omega} ^T}\pmb{x} + b} \right|}}{{\sqrt {\left\| \pmb{x} \right\|_2^2} }} 
	= \frac{{\left| {{\pmb{\omega} ^T}\pmb{x} + b} \right|}}{{\sqrt {\sum\limits_{i = 1}^d {\pmb{\omega} _i^2} } }}
\end{align*}
margin是数据点到判别面的最短距离，即，
\begin{align*}
	margin = 
	\arg\min d\left( \pmb{x} \right) 
	= \arg\min \frac{{\left| {{\pmb{\omega} ^T}\pmb{x} + b} \right|}}{{\sqrt {\sum\limits_{i = 1}^d {\pmb{\omega} _i^2} } }}
\end{align*}
而SVM的训练目标是最大化margin，于是，
\begin{align*}
	\begin{array}{l}
		\arg \mathop {\max }\limits_{\pmb{\omega} ,b} \arg \mathop {\min }\limits_{{x_i} \in D} \frac{{\left| {{\pmb{\omega} ^T}\pmb{x} + b} \right|}}{{\sqrt {\sum\limits_{i = 1}^d {\pmb{\omega} _i^2} } }}\\
		s.t.\forall {x_i} \in D,{y_i}\left( {{\pmb{\omega} ^T}\pmb{x} + b} \right) \ge 0
		\end{array}
\end{align*}
加强margin约束最小为1，则该优化可以转化为，
\begin{align*}
	\begin{array}{l}
		\arg \mathop {{\rm{min}}}\limits_{\pmb{\omega} ,b} \sqrt {\sum\limits_{i = 1}^d {\pmb{\omega} _i^2} } \\
		s.t.\forall {x_i} \in D,{y_i}\left( {{\pmb{\omega} ^T}\pmb{x} + b} \right) \ge 1
		\end{array}
\end{align*}

\section*{5 Hinge Loss在SVM中的意义}
Hinge Loss为：
\begin{align*}
	\arg \mathop {{\rm{min}}}\limits_{f \in H} \frac{1}{n}{\sum\limits_{i = 1}^n 
	{\left( {1 - {y_i}f\left( {{x_i}} \right)} \right)} _ + } 
	+ \lambda \left\| f \right\|_H^2
\end{align*}

对于线性不可分的两类数据，
需要使用soft-margin SVM。
也就是在对约束条件进行一个松弛，即
$\forall {x_i} \in D,{y_i}\left( {{\pmb{\omega} ^T}\pmb{x} + b} \right) \ge 1-\epsilon$，且
要在目标函数中最小化$\sum\limits_{i = 1}^n {\varepsilon _i}$。但对于margin之外的数据点，
进行松弛会使目标函数趋于负无穷，所以要对$\epsilon$进行限制，使得对于margin外的数据loss为0，
margin内的数据满足松弛条件。

\newpage
\section*{第二部分：计算机编程}
运行环境：python3.7.5

依赖库：scikit-learn numpy

实验中选取3和8两类数字进行分类，并按4：6的比例划分测试集与训练集。
之后使用scikit-learn中的svm进行训练，参数按照默认参数，即使用RBF核，
$\gamma = 1/n\_features$。

得到的实验结果如图1显示：
\begin{figure}[ht]
	\centering
	\includegraphics[width=1\textwidth]{1.jpg}
	\caption{SVM运行结果}
	\label{figl}
\end{figure}

准确率达到了0.99。
\end{document}
