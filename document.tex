\documentclass{article}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{fontspec}
\usepackage{graphicx}
\usepackage{subfigure}
\setmainfont[Mapping=tex-text]{KaiTi}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1}



%配置区
\newcommand{\courseName}{模式识别}
\newcommand{\homeworkId}{\#6} %作业编号
\newcommand{\homeworkTitle}{作业\homeworkId}
\newcommand{\studentId}{201928014629008}%学号
\newcommand{\studentName}{牛李金梁}%姓名

\newcommand{\question}[1]{\section*{Question #1}}
\renewcommand{\part}[1]{\subsection*{(#1)}}

\pagestyle{fancy}
\lhead{\studentName}
\rhead{\courseName\homeworkTitle}
\cfoot{\thepage}

\title{
    \vspace{2in}
    \textmd{\textbf{\courseName}:\homeworkTitle}\\
    \vspace{0.1in}
    \large{\studentId}\\
    \large{\studentName}\\
    \vspace{3in}
}

\begin{document}


\maketitle
\date{}
\pagebreak

\section*{第一部分：计算与证明}
\question{1 Adaboost算法的设计思想}
计算步骤：
\begin{itemize}
	\item[-] 初始化网络。通常使用随机初始化的方法。
	\item[-] 输入训练样本，每个样本为一个$d$维向量。
	\item[-] 计算映射层的权重向量和输入向量的距离。
		${d_j} = \sqrt {\sum\limits_{i = 1}^d {{{\left( {{x_i} - {w_{ij}}} \right)}^2}} } $。
	\item[-] 选择与权重向量距离最小的神经元作为胜出神经元${j^*}$，并给出其邻接
		神经元集合$h\left( {.,{j^*}} \right)$
	\item[-] 调整胜出神经元和其邻接神经元的权重，按下式更新：
	$$\Delta {w_{ij}} = \eta h\left( {j,{j^*}} \right)\left( {{x_i} - {w_{ij}}} \right)$$
	$${w_{ij}}\left( {t + 1} \right) = {w_{ij}}\left( t \right) + \Delta {w_{ij}}$$
	\item[-] 检查是否达到预设要求，是则结束算法，否则进行迭代。
\end{itemize}
\begin{itemize}
	\item[\pmb{没有免费的午餐定理}] 
	对寻找代价函数极值的算法，在平均到所有可能的代价函数上时其表现恰好相同。
	不存在一个与具体应用无关的、普遍适用的最优分类器。
	算法必须要引入一些与问题领域有关的假设。
	要想在某些指标上得到性能的提高，必须在另一些指标上付出相应的代价。
	\item[丑小鸭定理] 不存在分类的客观标准,一切分类的标准都是主观的。
	不存在与问题无关的最优的特征/属性集合。
	不存在与问题无关的模式之间的相似性度量 。
	\item[Occam 剃刀原理]如无必要，勿增实体。
	如果对训练数据分类的效果相同，简单的分类器往往优于复杂的分类器。
	相比复杂的假设，我们更倾向于选择简单的、参数少的假设。
	\item[最小描述长度原理]我们必须使模型的算法复杂度以及与该模型相适应
	的训练数据的描述长度之和最小。即，应该选择尽可能简单的分类器或模型。
\end{itemize}

算法流程图：
\begin{figure}[ht]
	\centering
	\includegraphics[width=1\textwidth]{Figure_2.png}
	\caption{自组织算法流程图}
	\label{figl}
\end{figure}

\question{3}
\part{1}

\begin{description}
	\item [\pmb{输入}] 图像大小：${\rm{400}} \times {\rm{400}}$
	\item [\pmb{第一卷积层}] 
	
	滤波器大小：${\rm{5}} \times {\rm{5}}$

	结点图像个数：$20$

	图像大小：${\rm{396}} \times {\rm{396}}$
	
	pooling后图像大小：${\rm{198}} \times {\rm{198}}$
	
	包含偏置的训练权重个数：${\rm{5}} \times {\rm{5}} \times {\rm{20 + 20 = 520}}$

	\item [\pmb{第二卷积层}] 
	滤波器大小：${\rm{3}} \times {\rm{3}} \times {\rm{20}}$

	结点图像个数：$30$

	滤波器总数：$30$

	图像大小：${\rm{196}} \times {\rm{196}}$
	
	pooling后图像大小：${\rm{98}} \times {\rm{98}}$
	
	包含偏置的训练权重个数：${\rm{20}} \times {\rm{3}} \times {\rm{3}} \times {\rm{30 + 30 = 5430}}$
	
	\item[\pmb{第三卷积层}]	
	滤波器大小：${\rm{3}} \times {\rm{3}} \times {\rm{30}}$

	结点图像个数：$20$

	滤波器总数：$20$
	
	图像大小：${\rm{96}} \times {\rm{96}}$
	
	pooling后图像大小：${\rm{48}} \times {\rm{48}}$
	
	包含偏置的训练权重个数：${\rm{30}} \times {\rm{3}} \times {\rm{3}} \times {\rm{20 + 20 = 5420}}$
	
	\item[\pmb{第四卷积层}]
	滤波器大小：${\rm{3}} \times {\rm{3}} \times {\rm{10}}$

	结点图像个数：$10$

	滤波器总数：$10$

	特征图尺度：${\rm{46}} \times {\rm{46}}$
	
	pooling后图像大小：${\rm{23}} \times {\rm{23}}$
	
	包含偏置的训练权重个数：${\rm{20}} \times {\rm{3}} \times {\rm{3}} \times {\rm{10 + 10 = 1810}}$

	\item[\pmb{全连接层}]
	输入图像大小： ${\rm{23}} \times {\rm{23}}$

	输入图像个数：$10$

	输出结点个数：$10$
	
	包含偏置的训练权重个数：${\rm{10}} \times {\rm{23}} \times {\rm{23}} \times {\rm{10 + 10 = 52910}}$
\end{description}
采用权值共享和局部连接的总权重个数为65290。
\\\\
\pmb{采用全连接网络}：

输入层到第一隐层：${\rm{400}} \times {\rm{400}} \times {\rm{396}} \times {\rm{396}} \times {\rm{20 + 396}} \times {\rm{396}} \times {\rm{20 = 501814336320}}$

第一隐层到第二隐层：${\rm{198}} \times {\rm{198}} \times {\rm{20}} \times {\rm{196}} \times {\rm{196}} \times {\rm{30 + 196}} \times {\rm{196}} \times {\rm{30 = 903637670880}}$

第二隐层到第三隐层：${\rm{98}} \times {\rm{98}} \times {\rm{30}} \times {\rm{96}} \times {\rm{96}} \times {\rm{20 + 96}} \times {\rm{96}} \times {\rm{20 = 53106462720}}$

第三隐层到第四隐层：${\rm{48}} \times {\rm{48}} \times {\rm{20}} \times {\rm{46}} \times {\rm{46}} \times {\rm{10 + 46}} \times {\rm{46}} \times {\rm{10 = 975073960}}$

第四隐层到输出层：${\rm{23}} \times {\rm{23}} \times {\rm{10}} \times {\rm{10 + 10}} \times {\rm{10 = 53000}}$

总权重个数约为${\rm{1}}{\rm{.459}} \times {\rm{1}}{{\rm{0}}^{{\rm{12}}}}$。
\\\\
\pmb{不采用权值共享}：

输入层到第一隐层：${\rm{5}} \times {\rm{5}} \times {\rm{396}} \times {\rm{396}} \times {\rm{20 + 396}} \times {\rm{396}} \times {\rm{20 = 105304320}}$

第一隐层到第二隐层：${\rm{3}} \times {\rm{3}} \times {\rm{20}} \times {\rm{196}} \times {\rm{196}} \times {\rm{30 + 196}} \times {\rm{196}} \times {\rm{30 = 208598880}}$

第二隐层到第三隐层：${\rm{3}} \times {\rm{3}} \times {\rm{30}} \times {\rm{96}} \times {\rm{96}} \times {\rm{20 + 96}} \times {\rm{96}} \times {\rm{20 = 49950720}}$

第三隐层到第四隐层：${\rm{3}} \times {\rm{3}} \times {\rm{20}} \times {\rm{46}} \times {\rm{46}} \times {\rm{10 + 46}} \times {\rm{46}} \times {\rm{10 = 3829960}}$

第四隐层到输出层：${\rm{23}} \times {\rm{23}} \times {\rm{10}} \times {\rm{10 + 10}} \times {\rm{10 = 53000}}$

总权重个数约为${\rm{3}}{\rm{.67}} \times {\rm{1}}{{\rm{0}}^{\rm{8}}}$。
\\\\
与二者相比，采用权值共享和局部连接的总权重个数基本可以忽略。

\part{2}
max pooling的前向传播是舍去其他元素，把区域内最大的值传给后一层。
反向传播则把梯度直接传给前一层该区域的某一个像素，
而其他像素为0。
确定这个像素需要记录下pooling时取的是哪个像素，
即最大值所在位置。

\part{3}
能改变网络结构的因素：
\begin{itemize}
	\item[-] 改变网络层数
	\item[-] 采用不同大小的滤波器
	\item[-] 激励函数的选择
	\item[-] 池化操作的选择
	\item[-] 能量函数的选择
\end{itemize}

\section*{第二部分：计算机编程}
环境为python3.7

依赖库：numpy、matplotlib

运行方式：直接在cmd中执行相应py文件

\question{1}
对于3层前向神经网络并使用反向传播进行训练，要确定以下参数：
\begin{itemize}
	\item[-] 批大小
	\item[-] 隐层结点个数
	\item[-] 梯度更新步长
	\item[-] 训练次数
\end{itemize}
还要根据已知参数确定输入层结点个数（样本维度），输出层类别个数。

具体训练中，先随机初始化层间权重，接着进行多次迭代。将数据分批送入
网络前向传播，并根据误差函数（平方误差）计算误差，然后将误差反向
传播来修正层间权值。经过迭代，误差越来越小，直到迭代次数满足预设的训练次数，
则根据预测判断正确率。

\question{2}
\part{a}
隐含层结点个数从1到10逐步增加，计算误差和训练集准确率，得到下面的图。
\begin{figure}[ht]
	\centering
	\subfigure{
		\begin{minipage}[t]{0.45\linewidth}
			\centering
			\includegraphics[width=3in]{a_1.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.45\linewidth}
			\centering
			\includegraphics[width=3in]{a_2.png}
		\end{minipage}
	}
	\centering
	\caption{隐含层结点个数对训练精度的影响}
	\label{figl}
\end{figure}

从图可知，随着隐含层结点数目的增加，训练误差整体趋势是减小，分类准确率整体趋势是增加。
但也可以看出，结点数并不需要无限制的增加，只需要采用适当的结点数，便可以有效地进行训练了。

\part{b}
设定隐含层结点数为8，梯度更新步长从0.002到0.2以0.002的步长变化，得到下面的图。
\begin{figure}[ht]
	\centering
	\subfigure{
		\begin{minipage}[t]{0.45\linewidth}
			\centering
			\includegraphics[width=3in]{b_1.png}
		\end{minipage}
	}
	\subfigure{
		\begin{minipage}[t]{0.45\linewidth}
			\centering
			\includegraphics[width=3in]{b_2.png}
		\end{minipage}
	}
	\centering
	\caption{梯度更新步长对训练的影响}
	\label{figl}
\end{figure}

从图中可以看出，随着更新步长的增加，训练误差呈先减小后增加的态势，
训练准确率也呈先提高后减小的态势。

出现这种情况是因为以较小的更新步长进行训练时，在有限迭代次数内仍未
收敛，所以此时的误差较高准确率较低。而对于较大的更新步长，由于更新步长过大，
所得的解一直在最优值附近徘徊却无法到达，因此误差和准确率出现了震荡。

因此，网络训练中应采取合适的更新步长，过小则收敛慢，过大则效果不好。

\part{3}
本实验将epoch设为1000，采用批量更新的方法，batch size设为10，即总共进行
3000次迭代，隐含层结点数为8，学习率为0.1，结果如下图所示。
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{c.png}
	\caption{目标函数随迭代次数增加的变化}
	\label{figl}
\end{figure}
从图中可以看出，本程序有良好的收敛特性。
\end{document}